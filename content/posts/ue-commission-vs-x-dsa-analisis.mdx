---
title: "Análisis Jurídico: La Multa de 120 Millones de € de la UE a X por Incumplir la DSA"
description: "Análisis exhaustivo del procedimiento sancionador de la Comisión Europea contra X (Twitter) por incumplimiento de la Ley de Servicios Digitales: dark patterns, transparencia publicitaria y acceso a datos para investigadores."
author: "Firma Scarpa"
date: "2026-02-26"
category: "Firma Scarpa"
tags: ["DSA", "Ley de Servicios Digitales", "X", "Twitter", "Comisión Europea", "Multa", "Elon Musk", "Dark Patterns", "CNMC", "Transparencia Algorítmica", "Soberanía Digital"]
image: "/images/firma-scarpa/ue-commission-x-dsa.jpg"
slug: "ue-commission-vs-x-dsa-analisis-juridico"
excerpt: "Primer análisis jurídico completo del caso que marcó un hito en la aplicación de la DSA: la multa de 120 millones de euros a X por diseño engañoso, opacidad publicitaria y bloqueo de datos a investigadores."
---

# Análisis Jurídico: La Multa de 120 Millones de € de la UE a X por Incumplir la DSA

## Capítulo 1: Evolución Normativa y Soberanía Tecnológica en la Unión Europea

### 1.1. Del régimen de responsabilidad limitada a la diligencia debida

El ecosistema de los servicios de la sociedad de la información ha experimentado una metamorfosis estructural desde la promulgación de la Directiva 2000/31/CE, transitando de un modelo centrado en el fomento del comercio electrónico a uno que reconoce la centralidad de los servicios intermediarios en la vida ciudadana y la economía  c . El marco jurídico actual, cristalizado en el Reglamento (UE) 2022/2065 (en adelante, DSA), impone un cambio de paradigma: se desplaza la exención condicionada de responsabilidad —históricamente estática bajo la doctrina del  safe harbour — hacia una obligación proactiva de  diligencia debida . Este nuevo régimen exige que los prestadores se comporten de modo "responsable y diligente" para garantizar un entorno en línea predecible y seguro. La transición no es meramente procedimental; representa un desplazamiento "de la responsabilidad a la obligación" ( from liability to duty ), donde el incumplimiento de las normas de moderación y gestión de riesgos conlleva sanciones sustantivas, independientemente de la ilicitud intrínseca del contenido alojado.

### 1.2. El marco nacional de adaptación: La gobernanza democrática en España

En el contexto español, la adaptación normativa se articula a través del "Anteproyecto de Ley para la Mejora de la Gobernanza Democrática en Servicios Digitales y Medios de Comunicación". Esta disposición legislativa es imperativa para asegurar la  implementación efectiva  del Derecho de la Unión, depurando el ordenamiento nacional de antinomias derivadas de la Ley 34/2002 (LSSI). La arquitectura de ejecución nacional se sustenta en la designación de la  Comisión Nacional de los Mercados y la Competencia (CNMC)  como el Coordinador de Servicios Digitales en España. Bajo esta nueva estructura, la autoridad nacional asume competencias de investigación y ejecución, incluyendo potestades sancionadoras por infracciones de los deberes de transparencia y diligencia, garantizando así una supervisión que la normativa previa no permitía de manera armonizada.

### 1.3. Objetivos de soberanía tecnológica y mitigación de niveles de riesgo

La DSA constituye una piedra angular en la estrategia de  soberanía tecnológica  de la Unión Europea, diseñada para contrarrestar el "control monopólico" ejercido por corporaciones extracomunitarias. El legislador europeo identifica a las plataformas de muy gran tamaño (VLOPs) no solo como agentes económicos, sino como fuentes de  riesgos sistémicos  que afectan la seguridad pública, la salud y los procesos electorales. La regulación establece distintos  niveles de riesgo , imponiendo obligaciones asimétricas: a mayor alcance de la plataforma, mayor es la carga de responsabilidad para mitigar efectos negativos en el discurso cívico. Este enfoque busca prevenir que el diseño de los sistemas, a menudo basados en  algoritmos opacos , degrade la autonomía de los ciudadanos o facilite la manipulación de la información.

### 1.4. Principio de Seguridad Jurídica y salvaguarda de Derechos Fundamentales

La integración del Derecho de la Unión en el ordenamiento interno debe regirse por el principio de  seguridad jurídica , eliminando incertidumbres que puedan afectar tanto a operadores como a ciudadanos. El marco de la DSA está fundamentado en el respeto absoluto a los  derechos fundamentales  consagrados en la Carta de los Derechos Fundamentales de la Unión Europea, con especial énfasis en la libertad de expresión, la no discriminación y la protección del consumidor. No obstante, la interrelación entre la IA y el derecho plantea tensiones críticas. El uso de sistemas automatizados para la moderación de contenidos, si no se somete a una transparencia plena de su código fuente, corre el riesgo de incurrir en una "censura colateral" o en la erosión de las libertades civiles. Por tanto, la normativa prescribe que toda limitación de derechos debe superar el "Test de Estrasburgo": debe ser prevista por la ley, perseguir un fin legítimo y ser necesaria y proporcionada en una sociedad democrática.

## Capítulo 2: Arquitectura de la Interfaz y Patrones Engañosos ( Dark Patterns )

### 2.1. Análisis técnico del Artículo 25 de la DSA: Prohibición del diseño engañoso

El régimen de gobernanza de las interfaces digitales bajo el Reglamento (UE) 2022/2065 establece una prohibición imperativa: los prestadores de plataformas en línea tienen proscrito diseñar, organizar u operar sus interfaces de modo que engañen o manipulen a los destinatarios del servicio. Esta disposición técnica persigue la neutralización de los denominados  patrones engañosos  ( dark patterns ), prácticas que distorsionan o menoscaban, ya sea de forma intencionada o efectiva, la capacidad de los usuarios para tomar decisiones autónomas e informadas. La norma prescribe que la arquitectura del sistema no debe "empujar" ( nudge ) al usuario hacia acciones que beneficien los intereses comerciales del prestador en detrimento de la autonomía del destinatario, salvaguardando así los  derechos fundamentales  a la libre elección y la transparencia.

### 2.2. El caso de la "Marca de Verificación Azul": Desmantelamiento de la confianza sistémica

La Comisión Europea ha identificado una infracción sustantiva del Artículo 25(1) en la reconfiguración de la "marca de verificación azul" de la plataforma X. Históricamente, este distintivo funcionaba como un indicador de autenticidad y notabilidad, basado en un proceso de verificación proactiva de la identidad frente al riesgo de suplantación. El modelo actual, implementado tras el cambio de propiedad en 2022, sustituyó este estándar por un sistema de suscripción pagada (X Premium) que otorga el mismo signo iconográfico sin una verificación de identidad significativa. Esta arquitectura de interfaz incurre en una  apropiación indebida de estándares visuales , al mantener un distintivo que el usuario medio asocia con la veracidad histórica para cuentas que solo han satisfecho un requisito financiero. La Comisión concluye que este diseño es inherentemente engañoso, ya que impide que el consumidor juzgue de manera informada la autenticidad del contenido con el que interactúa, distorsionando la percepción de fiabilidad en el ecosistema digital.

### 2.3. Amplificación Algorítmica Artificial y distorsión del discurso cívico

La arquitectura técnica de X integra el diseño de la interfaz con la  amplificación algorítmica  de las respuestas de los suscriptores de pago. Bajo el sistema de "priorización de respuestas", los suscriptores de X Premium reciben una visibilidad superior en los hilos de comentarios sin que el sistema informe de manera transparente que dicha relevancia es producto de un acuerdo comercial y no de métricas orgánicas o compromiso genuino. Este mecanismo utiliza  algoritmos opacos  para simular artificialmente la  notabilidad  de ciertos actores, lo que distorsiona el flujo de atención en el discurso público. Al tratar el contenido de forma diferenciada en el ranking algorítmico sin una advertencia clara de su naturaleza publicitaria o pagada, el prestador manipula la capacidad del usuario para discernir la relevancia real de la información, incrementando los  niveles de riesgo  para la integridad de los procesos de comunicación democrática.

### 2.4. Riesgo de abuso adversarial y facilitación de fraudes de suplantación

El debilitamiento de los estándares de verificación ha catalizado riesgos críticos de  abuso adversarial  por parte de actores maliciosos. La carencia de un filtrado ex-ante riguroso permite que defraudadores adquieran el "barniz de autenticidad" que otorga la marca azul para ejecutar estafas financieras, incluyendo fraudes de criptomonedas y suplantación de identidades institucionales. La evidencia técnica recabada demuestra que redes de desinformación estratégica explotan la confusión del usuario sobre el significado actual de la verificación para diseminar contenido dañino con una apariencia de legitimidad. En este sentido, la interfaz de la plataforma deja de ser un espacio neutral para convertirse en un facilitador técnico de engaños a gran escala, incumpliendo el deber de mitigación de riesgos sistémicos para la seguridad pública y los derechos de los consumidores.

## Capítulo 3: Transparencia Publicitaria y Cajas Negras Algorítmicas

### 3.1. Obligaciones de los repositorios de anuncios (Art. 39): Requisitos de fiabilidad

El Artículo 39(1) del Reglamento (UE) 2022/2065 impone a los prestadores de plataformas de muy gran tamaño (VLOPs) la obligación positiva de compilar y hacer público un repositorio de anuncios que sea accesible a través de una herramienta  searchable  y fiable. La norma prescribe que dicho repositorio debe permitir consultas multicriterio y ser accesible mediante interfaces de programación de aplicaciones (API), garantizando que la información sea exacta y completa durante todo el periodo de exhibición y hasta un año después de su última presentación. Este mandato busca mitigar los  niveles de riesgo  sistémico asociados a los sistemas publicitarios, los cuales poseen una capacidad intrínseca para amplificar desinformación o técnicas manipuladoras con impactos negativos en la salud pública y los procesos electorales.

### 3.2. Deficiencias sustantivas en el repositorio de X: Omisión de datos críticos

La investigación técnica ha determinado que el repositorio de anuncios de la plataforma X infringe sistemáticamente los estándares de transparencia al omitir información esencial para el escrutinio público. Específicamente, los informes generados carecen de datos sobre la entidad legal que financia el anuncio y el contenido real del mismo, limitándose en este último caso a proporcionar un localizador de recursos uniforme (URL) que puede ser alterado o eliminado por el anunciante, lo que anula la  perpetuidad de la prueba  exigida por la ley. Asimismo, se ha detectado que el sistema no permite identificar de manera fehaciente si el anuncio fue pagado por una persona física o jurídica distinta del anunciante, lo que constituye una opacidad operativa que impide detectar campañas de amenazas híbridas u operaciones de información coordinadas.

### 3.3. Barreras técnicas y opacidad operativa: El uso de algoritmos de retardo

La arquitectura del repositorio de X incorpora barreras de acceso que la Comisión Europea califica como decisiones de diseño deliberadas para frustrar la investigación independiente. Las pruebas periciales han demostrado que la herramienta impone un retraso artificial de tres minutos y veinte segundos para procesar cada informe, una latencia que no responde a limitaciones técnicas de la infraestructura, sino a una configuración que obliga al navegador a realizar consultas de estado repetitivas antes de liberar el archivo. Adicionalmente, el acceso a través de API resulta disfuncional en la práctica; los investigadores encuentran errores sistemáticos (Código 453) incluso tras haber satisfecho suscripciones comerciales costosas, lo que de facto anula el canal institucional de transparencia algorítmica prescrito por la DSA.

### 3.4. IA Generativa y Grok: Riesgos de desinformación y contenido sintético

En el marco de la supervisión de riesgos sistémicos, la Comisión ha incoado procedimientos adicionales para evaluar el impacto de la IA generativa  Grok  y sus sistemas de recomendación. La preocupación regulatoria se centra en la difusión de contenido sexual explícito manipulado mediante IA ( deepfakes ) y el potencial de estos  algoritmos opacos  para degradar los  derechos fundamentales  de mujeres y menores. La falta de controles robustos sobre el contenido sintético y la insuficiente supervisión humana en la moderación de respuestas generadas por Grok elevan el perfil de riesgo de la plataforma, especialmente en lo relativo a la protección del bienestar mental y la integridad del discurso cívico frente a la manipulación algorítmica a gran escala.

## Capítulo 4: Acceso a Datos y la Función Social de la Investigación

### 4.1. El Artículo 40 de la DSA como derecho subjetivo: Acceso a datos públicos en tiempo real

La arquitectura de transparencia del Reglamento (UE) 2022/2065 trasciende la mera rendición de cuentas pasiva para instituir un  derecho subjetivo  al acceso de datos en favor de la comunidad investigadora. El Artículo 40(12) impone a los prestadores de plataformas de muy gran tamaño (VLOPs) la obligación imperativa de facilitar el acceso, sin demora indebida y de forma gratuita, a datos que sean públicamente accesibles en su interfaz, incluyendo datos en tiempo real cuando sea técnicamente factible. Este precepto es autónomo y directamente aplicable, no estando supeditado a la adopción de actos delegados ni a la intermediación de los Coordinadores de Servicios Digitales (DSC). La finalidad última de este régimen es corregir las  asimetrías de información  y permitir un escrutinio independiente sobre la evolución y gravedad de los  riesgos sistémicos  en la Unión.

### 4.2. Obstáculos contractuales y el conflicto del scraping: Análisis de la prohibición de X

El cumplimiento del Artículo 40(12) exige que los prestadores no solo proporcionen herramientas técnicas como las API, sino que también se abstengan de imponer barreras contractuales que prohíban el acceso independiente. La investigación técnica sobre la plataforma X revela que sus términos de servicio prohíben de forma generalizada el uso de técnicas automatizadas como el  data scraping  o el crawling. Esta restricción contractual constituye una infracción del mandato de "no impedir" el uso de datos públicos para la investigación de riesgos. Las técnicas de acceso independiente son calificadas por el regulador como instrumentos indispensables para auditar los  algoritmos opacos  de recomendación y verificar la calidad de la información suministrada a través de los canales oficiales del prestador.

### 4.3. Infraestructura y el rol de los coordinadores (DSC): La posición de Coimisiún na Meán

La gestión de solicitudes para el acceso a datos no públicos bajo el Artículo 40(4) recae en una estructura descentralizada donde los DSC actúan como intermediarios necesarios entre los investigadores y las plataformas. Dado que la mayoría de las VLOPs tienen su sede principal en Irlanda, la  Coimisiún na Meán  asume una responsabilidad estratégica y desproporcionada en la tramitación y validación de credenciales de los investigadores cualificados. Esta infraestructura institucional busca armonizar los estándares de seguridad y confidencialidad, asegurando que las solicitudes sean proporcionadas y protejan debidamente los secretos comerciales del prestador. No obstante, la falta de plazos legales para concluir los procedimientos de infracción y la dependencia de la cooperación de las plataformas generan un entorno de incertidumbre que puede comprometer la eficacia del sistema.

### 4.4. Impacto en la detección de riesgos sistémicos: El efecto desalentador del bloqueo de datos

La imposición de barreras financieras y procesales por parte de X ha generado un  efecto desalentador  ( chilling effect ) que ha forzado la cancelación o suspensión de centenares de proyectos de investigación sobre desinformación y discurso de odio. Específicamente, el desplazamiento de investigadores hacia niveles de pago prohibitivos (v.gr., USD 5,000 mensuales) y la reducción de las cuotas de datos en un 1000% —comparado con el programa académico previo— asfixian la capacidad de detectar campañas coordinadas de manipulación. Al rechazar solicitudes basándose en interpretaciones restrictivas de la ubicación geográfica o la afiliación institucional, el prestador degrada la vigilancia social necesaria para mitigar los  niveles de riesgo  que afectan a la integridad de los procesos electorales y la seguridad pública en la Unión.

## Capítulo 5: Supervisión, Ejecución y Régimen Sancionador

### 5.1. Autoridades Competentes: La Arquitectura de Supervisión en España

La ejecución del Reglamento (UE) 2022/2065 (DSA) se articula mediante una estructura de gobernanza descentralizada que impone a los Estados miembros la obligación de designar autoridades independientes de supervisión. En el ordenamiento jurídico español, el  Coordinador de Servicios Digitales (DSC)  es la  Comisión Nacional de los Mercados y la Competencia (CNMC) , a la cual se le atribuye la responsabilidad de controlar el cumplimiento de las obligaciones de diligencia debida. Esta competencia se ejerce de forma concurrente con la  Agencia Española de Protección de Datos (AEPD) , la cual asume de manera específica el control de las prohibiciones relacionadas con la publicidad basada en el perfilado de menores y el tratamiento de categorías especiales de datos personales bajo los Artículos 26.3 y 28.2 del Reglamento. La CNMC, a través de su Dirección de Servicios Digitales, actúa como el nodo central en la red de supervisión europea, colaborando estrechamente con la Comisión Europea y otros DSC, particularmente el irlandés  Coimisiún na Meán , dado que gran parte de las VLOPs tienen su sede en dicha jurisdicción.

### 5.2. El Procedimiento Sancionador: El Caso de X y la Unidad Económica Única

El procedimiento de infracción contra la plataforma X constituye el primer hito jurisprudencial en la aplicación coercitiva de la DSA. El 5 de diciembre de 2025, la Comisión Europea impuso una sanción pecuniaria de  120 millones de euros  tras constatar incumplimientos graves en materia de transparencia y diseño de interfaz. La decisión es doctrinalmente significativa por la aplicación del principio de  Unidad Económica Única , extendiendo la responsabilidad solidaria no solo a XIUC (antes TIUC), sino también a X Holdings Corp., X.AI Holdings Corp. y, de manera personal, a  Elon Musk . La gravedad de la infracción se fundamenta en el impacto sobre más de 109 millones de usuarios mensuales en la Unión, lo que eleva el  nivel de riesgo  sistémico para el discurso cívico y la integridad de los procesos democráticos. El desglose de la multa —45 millones por el diseño engañoso de la marca azul, 35 millones por el repositorio publicitario y 40 millones por el bloqueo de datos a investigadores— refleja una voluntad sancionadora disuasoria que puede alcanzar hasta el 6% de la facturación global anual.

### 5.3. Potestades de Inspección y Acceso a Algoritmos

Para garantizar la eficacia de la supervisión, el DSC y la Comisión gozan de potestades inspectoras invasivas que permiten penetrar la opacidad de los sistemas técnicos. Los funcionarios autorizados tienen el mandato legal de entrar en los locales de los prestadores y exigir acceso total a su  organización, sistema informático y algoritmos . Estas facultades incluyen el sellado de dependencias comerciales y el derecho a solicitar explicaciones técnicas detalladas sobre el funcionamiento de los sistemas de recomendación y moderación de contenidos. En el caso de que la inspección suponga una restricción del derecho fundamental a la inviolabilidad del domicilio, la normativa española prescribe la necesidad de una  autorización judicial  previa, la cual debe ser resuelta en un plazo máximo de 48 horas para no comprometer la celeridad de la investigación.

### 5.4. Garantías Procesales: El Derecho de Defensa y el Régimen de "Data Rooms"

La imposición de sanciones masivas exige un respeto escrupuloso a los  derechos fundamentales  de defensa y a la tutela judicial efectiva. Durante el procedimiento contra X, se suscitó un conflicto procesal relevante en torno al acceso al expediente. La Comisión implementó un sistema de  "Data Room" , permitiendo que únicamente asesores externos cualificados revisaran documentos confidenciales para proteger secretos comerciales, mientras que el acceso directo de la empresa se limitó a los documentos citados en las conclusiones preliminares. Aunque X alegó una vulneración del privilegio legal y una restricción indebida de su defensa, la Comisión defendió que este mecanismo equilibra la necesidad de una ejecución rápida de la ley con la protección de la confidencialidad de terceros. Cualquier decisión final de incumplimiento es recurrible ante el  Tribunal General de la Unión Europea , lo que garantiza que la discrecionalidad de la Comisión esté sujeta a un control jurisdiccional pleno.

## Capítulo 6: Conflictos de Derechos Fundamentales y Perspectiva Futura

### 6.1. Libertad de Expresión vs. Moderación de Contenidos: El riesgo de censura colateral

La implementación del Reglamento (UE) 2022/2065 (DSA) instituye una tensión dialéctica entre la seguridad del entorno digital y la preservación de la libertad de expresión, derecho consagrado en el Artículo 11 de la Carta de los Derechos Fundamentales de la UE. La doctrina advierte que la presión regulatoria para la retirada expeditiva de información, bajo el amparo de las cláusulas de "conocimiento efectivo" (Art. 16), puede inducir a los prestadores a un sobrebloqueo preventivo de contenido lícitamente protegido. Este fenómeno, caracterizado como  censura colateral , se ve agravado por el uso de  algoritmos opacos  y "cajas negras decisionales" que carecen de la sensibilidad contextual necesaria para distinguir entre el discurso abyecto y la crítica política legítima. En este sentido, críticos doctrinales han calificado estas disposiciones como "códigos de discurso extraterritoriales" que facultan a la Comisión para ejercer un control narrativo global, erosionando el pluralismo inherente a una sociedad democrática.

### 6.2. Independencia y Pluralismo Mediático: La convergencia con el Reglamento (UE) 2024/1083

La gobernanza de las plataformas digitales debe converger necesariamente con el marco de protección a los servicios de medios de comunicación independientes, articulado en el Reglamento (UE) 2024/1083 (Ley Europea de Libertad de los Medios de Comunicación). Este corpus normativo busca blindar la  independencia editorial  frente a la moderación arbitraria de las plataformas de muy gran tamaño (VLOPs), exigiendo salvaguardas específicas cuando el contenido restringido proviene de prestadores de medios sujetos a estándares de transparencia de propiedad y control de conflictos de interés. La normativa prescribe la creación de registros estatales de medios bajo la supervisión de la CNMC en España, asegurando que la lucha contra la desinformación no se traduzca en una restricción indebida del pluralismo mediático o en la manipulación de la relevancia informativa por parte de sistemas algorítmicos con intereses comerciales.

### 6.3. Predictibilidad Judicial en el entorno de la IA: Hacia una regulación basada en la evidencia

La transición hacia una gobernanza digital efectiva requiere desplazar la discrecionalidad administrativa hacia un modelo de  regulación basada en la evidencia . La integración de herramientas de análisis de datos para supervisar los  niveles de riesgo  sistémico permite que tanto los Coordinadores de Servicios Digitales como los investigadores acreditados identifiquen patrones de incumplimiento con rigor científico. Este enfoque empoderador busca corregir la  asimetría de información  entre las corporaciones tecnológicas y el poder público, facilitando una predictibilidad judicial que proteja la autonomía del usuario. No obstante, el éxito de este modelo depende de la transparencia completa de los algoritmos y de la capacidad del sistema legal para someter las decisiones automatizadas a un control humano real y no meramente formal.

### 6.4. Conclusiones y Vacíos Legales: Indeterminación procedimental y vaguedad conceptual

A pesar de su ambición, el marco regulatorio actual presenta incertidumbres críticas que comprometen la seguridad jurídica. En primer lugar, se observa una preocupante  indeterminación de plazos , dado que la DSA no prescribe un límite legal para la conclusión de los procedimientos formales de infracción, lo que puede derivar en situaciones de acoso administrativo persistente sin tutela judicial inmediata. En segundo lugar, el concepto de  riesgo sistémico  adolece de una vaguedad conceptual que otorga a la Comisión Europea una discrecionalidad excesiva para definir qué contenidos afectan a la seguridad pública o al discurso cívico. Finalmente, la coexistencia de las doctrinas de  safe harbour  y la "cláusula del buen samaritano" genera un vacío legal en el que las plataformas pueden actuar como censores privados para obtener inmunidad sancionadora, desplazando la función jurisdiccional de ponderación de derechos fundamentales hacia entidades privadas no sujetas a los mismos estándares de rendición de cuentas que el Estado.

---

## Fuentes analizadas (Consolidado)

### Referencias de los Capítulos 1 y 2:

- SENSITIVE - House Judiciary Committee. Índice y secciones sobre el incumplimiento del Artículo 25(1) por parte de X.
- SENSITIVE - House Judiciary Committee. Marco legal del Artículo 25(1) y definición de dark patterns.
- SENSITIVE - House Judiciary Committee. Antecedentes históricos de la verificación de Twitter y su transición al modelo de pago.
- SENSITIVE - House Judiciary Committee. Hallazgos preliminares de la Comisión Europea sobre el diseño engañoso de la marca azul.
- SENSITIVE - House Judiciary Committee. Análisis de la pérdida de estándares de identidad.
- SENSITIVE - House Judiciary Committee. Comparativa de la marca azul frente a estándares de la industria.
- SENSITIVE - House Judiciary Committee. Examen técnico de la amplificación algorítmica.
- SENSITIVE - House Judiciary Committee. Evidencia de abuso adversarial y riesgos de desinformación.
- SENSITIVE - House Judiciary Committee. Conclusión sobre incentivos financieros.

### Referencias del Capítulo 3:

- Can't Hide: Extraterritorial European Speech Codes. Investigaciones sobre Grok y sistemas de recomendación; evaluación de riesgos del Art. 34.
- Commission Fines X €120 Million / Commission Investigates Grok. Hallazgos sobre opacidad del repositorio de anuncios y deepfakes.
- Commission Opens Formal Proceedings Against X. Áreas de infracción y obligaciones de los Artículos 16, 25, 39 y 40.
- Governing Online Platforms After the DSA. Análisis de fallas en procesos de moderación.
- Informe Artículo 35.2 DSA. Riesgos de discriminación algorítmica y contenido sintético.
- SENSITIVE - House Judiciary Committee. Resultados técnicos sobre el repositorio de anuncios de X y retardos artificiales.

### Referencias del Capítulo 4:

- Data Access for Researchers under the Digital Services Act. Documento del Weizenbaum Institute.
- SENSITIVE - House Judiciary Committee. Multa de 40 millones y análisis de la prohibición del scraping en los términos de X.
- Elon Musk advierte a TikTok y Meta sobre el desafío X. Recurso ante el Tribunal General.
- The Regulation of Disinformation Under the DSA. Interrelación entre acceso a datos y libertad de expresión.
- Democracy Reporting International vs. X. Reconocimiento del Art. 40(12) como derecho subjetivo.

### Referencias del Capítulo 5:

- Anteproyecto de Ley para la mejora de la Gobernanza Democrática. Competencias de CNMC, AEPD y facultades de inspección.
- Comunicados de Prensa de la Comisión Europea. Detalles de la sanción de 120 millones.
- SENSITIVE - House Judiciary Committee (Decisión C(2025) 8630). Responsabilidad de Elon Musk y régimen de "Data Rooms".
- Selling, LK et al. (2025). Data Access for Researchers. Rol de los DSCs.

### Referencias del Capítulo 6:

- Anteproyecto de Ley XX/XXXX para la mejora de la Gobernanza Democrática. Integración del Reglamento (UE) 2024/1083.
- Price, Lorcán (Alliance Defending Freedom International). Testimonio sobre censura extraterritorial.
- Selling, LK et al. (2025). Data Access for Researchers under the DSA. Impacto del acceso a datos.
- Fabbri, Matteo (2024). Governing Online Platforms After the DSA. Procedimiento administrativo.
- Herce Maza, Jose Ignacio (2024). IA y Exención de Responsabilidad. Riesgos de algoritmos opacos.
- De Nova Labián, Alberto José (2024). La diligencia debida de las plataformas. Imposibilidad técnica de control total.
- Ó Fathaigh, Ronan et al. (2025). The Regulation of Disinformation. Sobrebloqueo y vaguedad del riesgo sistémico.
