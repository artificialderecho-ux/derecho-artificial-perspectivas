---
title: "Riesgos Visibles e Invisibles del Uso de Imágenes de Terceros en Sistemas de Inteligencia Artificial"
date: "2026-02-21"
category: "firma-scarpa"
tags: ["ia-generativa", "imagenes-terceros", "aepd-guia-2026", "proteccion-datos", "riesgos-invisibles", "rgpd-ia", "privacidad-imagenes", "etica-ia"]
pdf: "/fuentes/guia-aepd-uso-de-imagenes-de-terceros-en-sistemas-ia.pdf"
author: "Ricardo Scarpa"
description: "Análisis jurídico completo de la Guía AEPD 2026 sobre uso de imágenes de terceros en IA: riesgos visibles e invisibles, tratamiento de datos personales y obligaciones RGPD para responsables y proveedores."
image: "/images/firma-scarpa.jpg"
seoTitle: "Ia-generativa 2026: Riesgos Visibles e | Derecho Artificial"
seoDescription: "Análisis jurídico completo de la Guía AEPD 2026 sobre uso de imágenes."
keywords: ["Ia-generativa", "Imagenes-terceros", "Aepd-guia-2026"]
seoKeyword: "Ia-generativa"
---

Resumen Ejecutivo

El presente artículo analiza en profundidad la Guía de la Agencia
Española de Protección de Datos (AEPD) de enero de 2026 sobre el uso de
imágenes de terceros en sistemas de inteligencia artificial. Este
documento constituye una referencia fundamental para comprender los
riesgos legales, tanto visibles como invisibles, derivados del
tratamiento de datos personales en forma de imágenes mediante sistemas
de IA generativa.

La creciente utilización de herramientas de generación y manipulación de
imágenes con inteligencia artificial ha generado nuevos escenarios de
riesgo que van más allá de los daños aparentes inmediatos, afectando a
aspectos cruciales de la protección de datos personales que permanecen
ocultos para usuarios y personas afectadas.

La guía de la AEPD proporciona un marco normativo y técnico esencial
para evaluar estos riesgos desde una perspectiva de protección de datos
y derechos fundamentales. Los criterios analíticos que propone no
constituyen cambios en la doctrina de protección de datos, sino una
aplicación sofisticada de principios bien establecidos al contexto
específico de la inteligencia artificial generativa.

I. Introducción: El Contexto Normativo y Fáctico

En el contexto actual de proliferación de herramientas de inteligencia
artificial generativa, la utilización de imágenes o vídeos de terceros
ha adquirido una dimensión que trasciende los usos triviales. Una
fotografía, vídeo o representación visual en la que una persona es
identificada o identificable constituye un dato personal conforme a los
términos del Reglamento General de Protección de Datos (RGPD) y la Ley
Orgánica de Protección de Datos Personales (LOPDGDD).

La identificabilidad de una persona no se limita a su rostro. Conforme a
los criterios de la jurisprudencia especializada y la práctica
regulatoria internacional, una persona puede ser identificable mediante
su voz, cuerpo, gestos, vestimenta, tatuajes, entorno relacional o por
la combinación de varios elementos. Esta circunstancia se mantiene
incluso cuando la imagen ha sido alterada mediante sistemas de
inteligencia artificial.

La generalización del uso irreflexivo de herramientas de IA para generar
o modificar imágenes ha normalizado prácticas que, desde la perspectiva
de la protección de datos, presentan riesgos considerables. Estos usos
aparentemente banales (filtros, avatares, caricaturas, animaciones)
generan impactos reales tanto visibles como invisibles.

II\. Riesgos Visibles: Criterios de Evaluación Aplicables

Los riesgos denominados visibles son aquellos que se aprecian cuando una
imagen o vídeo generado o modificado mediante IA se comparte, publica o
difunde en cualquier medio. La AEPD establece ocho criterios
fundamentales que siguen siendo plenamente aplicables aunque el
contenido haya sido generado o modificado por sistemas de inteligencia
artificial.

A\) Expectativa Razonable y Legitimación para el Uso Concreto

Que una fotografía se encontrase disponible en un grupo de mensajería
privada, en una red social o hubiese sido enviada puntualmente no
confiere autorización general para incorporarla a herramientas de IA,
transformarla, generar variantes o difundir el resultado. Cuanto mayor
sea el distanciamiento respecto del contexto original, mayor debe ser la
exigencia de legitimación.

B\) Alcance y Facilidad de Difusión

El impacto potencial depende tanto del número de destinatarios como de
la facilidad técnica para su reenvío y copia. En redes sociales, los
mecanismos de amplificación algorítmica aceleran exponencialmente el
impacto, con independencia de si el contenido es generado por IA o
material original.

C\) Persistencia y Posibilidad Real de Retirar el Contenido

El riesgo se incrementa cuando resulta imposible retirar efectivamente
el contenido y sus copias subsecuentes. Las medidas de retirada no
siempre garantizan la eliminación total, por lo que la reversibilidad
práctica constituye un elemento central en la evaluación del riesgo.

D\) Sexualización y Contenido Íntimo Sintético

Esta categoría presenta una señal de riesgo de elevadísima intensidad.
Se refiere a desnudez añadida, erotización, insinuaciones sexuales o
escenas íntimas generadas a partir de imágenes neutrales. El daño
potencial es considerable, facilitando chantaje y acoso.

E\) Atribución de Hechos No Reales y Efectos Reputacionales

El riesgo surge cuando el contenido atribuye a una persona hechos o
conductas que no ocurrieron y resultan verosímiles. Esto afecta
significativamente la reputación, relaciones personales y posición
profesional. El perjuicio puede ser grave y exige valoración rigurosa.

F\) Descontextualización y Reinterpretación

Una imagen o vídeo pueden causar daño si se presentan fuera de contexto
o acompañados de textos que alteran su significado. Este riesgo no queda
neutralizado por los filtros de generación del proveedor.

G\) Vulnerabilidad de la Persona Afectada

El umbral de prudencia debe ser máximo con menores de edad, personas
mayores, con discapacidad u otros colectivos vulnerables. Incluso usos
aparentemente inocentes pueden desencadenar acoso o estigmatización.

H\) Impacto Especial en la Persona

Debe evaluarse si existe daño constatable en relaciones personales,
integridad física, ámbito laboral, daños psicológicos, contexto
educativo, suplantación de identidad o cualquier efecto negativo
jurídico significativo.

III\. Riesgos Invisibles: Dimensiones Ocultas del Tratamiento

Existen riesgos reales que afectan a la persona cuya imagen se carga en
un sistema de IA, simplemente por el hecho de hacerlo, incluso cuando el
uso es trivial y el resultado no es publicado. Estos riesgos derivan del
funcionamiento normal de los servicios y suelen pasar inadvertidos para
usuarios y afectados.

A\) Pérdida Efectiva de Control

Al cargar una imagen a un sistema de IA, el contenido cesa estar bajo
control del usuario y pasa a ser tratado por un proveedor externo. Para
la persona afectada, esto supone pérdida real de control sobre dónde
está su imagen y qué ocurre con ella.

B\) Retención Técnica y Copias No Visibles

Muchos sistemas de IA conservan temporalmente imágenes para
procesamiento, gestión de errores o copias de seguridad. Esta retención
suele ser invisible y no verificable, dificultando saber si la imagen
fue realmente eliminada.

C\) Intervención de Varios Actores

El tratamiento suele intervenir infraestructuras de nube, servicios de
almacenamiento, herramientas de seguridad y personal técnico. Esto
amplía el número de sistemas y personas que pueden acceder a la imagen,
incrementando significativamente el riesgo.

D\) Finalidades Propias y Ampliadas del Proveedor

Además del resultado solicitado, el proveedor puede tratar imágenes para
garantizar seguridad, detectar abusos, evaluar calidad del servicio o
mejorar funcionamiento. Esto implica conservar ejemplos durante más
tiempo o reutilizarlos internamente.

E\) Generación de Metadatos e Inferencias Internas

Durante el procesamiento, los sistemas analizan automáticamente el
contenido para detectar rostros, cuerpos o características, generando
metadatos técnicos. Aunque estos análisis tengan finalidad funcional,
constituyen tratamientos adicionales que dejan rastro.

F\) Riesgo de Identificación Persistente

Algunas herramientas están diseñadas para que una persona aparezca
coherentemente en varias imágenes generadas a partir de una sola
fotografía. El sistema reutiliza rasgos que permiten reconocer al
sujeto, facilitando reidentificación y pérdida de control.

G\) Asimetría Informativa y Dificultad de Ejercer Derechos

La persona afectada suele desconocer qué sistema se utilizó, qué sucedió
con su imagen, cuánto tiempo se conservó o a quién dirigirse para
solicitar supresión. Esta asimetría limita prácticamente el ejercicio de
derechos de acceso, supresión u oposición.

H\) Riesgo de Exposición por Incidentes de Seguridad

Los sistemas de IA pueden sufrir fallos técnicos, accesos indebidos o
brechas de seguridad. Las imágenes cargadas pueden quedar expuestas sin
difusión voluntaria, con impacto elevado para la persona afectada.

I\) Efecto Multiplicador y Conexión con Daños Posteriores

Una vez cargada, resulta sencillo generar múltiples variantes en poco
tiempo. Este bajo coste de repetición aumenta la probabilidad de
resultados lesivos en iteraciones posteriores, explicando por qué muchos
daños visibles se apoyan en riesgos técnicos previos.

**Cargar imágenes de terceros a sistemas de IA no es un acto neutral,
incluso sin difusión posterior ni intención de causar daño. Los riesgos
surgen de pérdida de control, opacidad técnica, finalidades ampliadas y
dificultad de reacción de la persona afectada, factores que deben
considerarse incluso en usos aparentemente banales.**

IV\. Situaciones Especialmente Relevantes para la AEPD

En muchos casos, los usos quedan fuera del ámbito de aplicación de la
normativa de protección de datos cuando se realizan de forma
estrictamente personal o doméstica. De igual modo, el tratamiento de
personas fallecidas no se encuentra comprendido en el ámbito de
aplicación del RGPD.

Sin embargo, esto no excluye que puedan verse afectados otros derechos
fundamentales como honor, intimidad o propia imagen, ni que resulten
aplicables otras normas del ordenamiento jurídico, incluyendo el Código
Penal. Cuando concurran indicios de delito, la actuación corresponde a
autoridades policiales, Fiscalía y órganos judicales.

Supuestos de Especial Atención

La AEPD presta especial atención cuando el uso incrementa
significativamente los riesgos para la persona afectada:

**1. Pérdida efectiva de control sobre la imagen con desconocimiento de
su uso.**

**2. Generación de contenidos verosímiles que atribuyen hechos no
ocurridos.**

**3. Implicación de menores de edad o personas vulnerables.**

**4. Introducción de elementos de sexualización, humillación o
descrédito.**

**5. Difusión en entornos donde el impacto personal, social o
profesional es especialmente intenso.**

V. Conclusiones y Perspectiva Jurídica

La Guía de la AEPD de enero de 2026 representa un hito importante en el
desarrollo de la jurisprudencia de protección de datos aplicable a
sistemas de inteligencia artificial. Su contribución radica en la
distinción clara entre riesgos visibles (apreciables en la difusión) y
riesgos invisibles (derivados del mero hecho de cargar una imagen).

Esta categorización supone un avance conceptual porque atiende a la
realidad operacional de los sistemas de IA actuales, muchos de los
cuales generan impactos independientemente de que el usuario publique o
comparta el resultado. La opacidad técnica, multiplicación de actores,
reutilización persistente y amplitud de finalidades operan
inevitablemente.

Desde la perspectiva de análisis jurídico, la Guía refuerza que
cualquier tratamiento de datos personales, incluyendo los realizados por
sistemas de IA, requiere base de legitimación suficiente conforme a
RGPD. La trivialidad aparente de un uso no exonera del cumplimiento de
estos principios constitucionales.

**La responsabilidad de proveedores de IA se ve reforzada: deben
implementar controles técnicos robustos, comunicar qué sucede con las
imágenes, cuánto se conservan y para qué finalidades. Los usuarios deben
desarrollar comprensión sofisticada de los riesgos reales, más allá de
la familiaridad de la herramienta.**

Documento de Análisis Jurídico

Basado en la Guía oficial de la AEPD

Enero 2026
