<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GLOSARIO DE TÉRMINOS IA SECTOR LEGAL - Derecho Artificial</title>
    <meta name="description" content="Glosario integral de términos clave en Inteligencia Artificial y Derecho, enfoque UE y España. Actualizado febrero 2026.">
    <style>
        body { font-family: Arial, Helvetica, sans-serif; line-height: 1.6; margin: 0; padding: 20px; max-width: 1000px; margin: auto; color: #333; background: #fff; }
        header { text-align: center; margin-bottom: 40px; }
        .logo { font-size: 2.8em; font-weight: bold; color: #003366; margin: 20px 0; }
        h1 { color: #003366; margin: 10px 0; }
        .indice { background: #f0f8ff; padding: 15px; border-radius: 8px; margin: 20px 0; text-align: center; }
        .indice a { margin: 0 8px; color: #0066cc; text-decoration: none; font-weight: bold; }
        .indice a:hover { text-decoration: underline; }
        section { margin-bottom: 50px; }
        h2 { color: #003366; border-bottom: 3px solid #003366; padding-bottom: 8px; font-size: 1.8em; }
        dt { font-weight: bold; color: #004080; margin-top: 25px; font-size: 1.2em; }
        dd { margin-left: 25px; margin-bottom: 20px; background: #f9f9f9; padding: 12px; border-left: 4px solid #0066cc; border-radius: 4px; }
        a { color: #0066cc; text-decoration: none; }
        a:hover { text-decoration: underline; }
        .ref { font-style: italic; color: #555; }
        footer { text-align: center; margin-top: 60px; padding: 20px; border-top: 1px solid #ccc; font-size: 0.95em; color: #555; }
    </style>
</head>
<body>

<header>
    <div class="logo">DERECHO ARTIFICIAL</div>
    <h1>GLOSARIO DE TÉRMINOS IA SECTOR LEGAL</h1>
    <p>Enfoque UE – España | Actualizado: febrero 2026</p>
    <p>Autor: Ricardo Scarpa | Derecho Artificial</p>
    <p><em>Balanza con cerebro humano y cerebro artificial</em></p>
</header>

<div class="indice">
    <strong>Índice alfabético:</strong><br>
    <a href="#A">A</a> | <a href="#B">B</a> | <a href="#C">C</a> | <a href="#D">D</a> | <a href="#E">E</a> | <a href="#F">F</a> | 
    <a href="#G">G</a> | <a href="#H">H</a> | <a href="#I">I</a> | <a href="#L">L</a> | <a href="#M">M</a> | 
    <a href="#O">O</a> | <a href="#P">P</a> | <a href="#R">R</a> | <a href="#S">S</a> | <a href="#T">T</a> | 
    <a href="#V">V</a> | <a href="#X">X</a>
</div>

<section id="A">
    <h2>A</h2>
    <dl>
        <dt>Acta de Inteligencia Artificial (AI Act)</dt>
        <dd>Reglamento (UE) 2024/1689 que establece el primer marco jurídico integral para la IA en la Unión Europea. Clasifica los sistemas según niveles de riesgo (inaceptable, alto, limitado, mínimo), prohíbe prácticas inaceptables e impone obligaciones de transparencia, supervisión y evaluación de conformidad. Es el eje normativo central del Derecho europeo de la IA.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689</a><br>
        <strong>Notas prácticas 2026:</strong> Prohibiciones vigentes desde 02.02.2025; obligaciones para modelos de propósito general desde 02.08.2025; obligaciones principales para sistemas de alto riesgo desde 02.08.2026.</dd>

        <dt>Acto administrativo automatizado / Actuación administrativa automatizada</dt>
        <dd>Decisión dictada por una Administración Pública mediante sistema automatizado sin intervención humana directa en cada caso concreto.<br>
        <strong>En España:</strong> Regulado explícitamente en el art. 41 de la Ley 40/2015. Requiere identificación del órgano responsable, determinación del sistema y garantías de control humano y revisión.<br>
        <strong>Punto crítico:</strong> Compatibilidad con el derecho a la tutela judicial efectiva (art. 24 CE).<br>
        <span class="ref">Referencia oficial:</span> <a href="https://www.boe.es/buscar/act.php?id=BOE-A-2015-10566">Ley 40/2015, art. 41</a>.</dd>

        <dt>Algoritmo</dt>
        <dd>Conjunto de instrucciones lógicas o matemáticas para procesar datos y producir resultados. En el ámbito jurídico, relevante como fuente de decisiones con efectos legales, especialmente cuando es opaco o automatizado.</dd>

        <dt>Algoritmo decisorio público</dt>
        <dd>Algoritmo utilizado por la Administración para adoptar decisiones con efectos sobre la ciudadanía. Existe debate doctrinal sobre si debe considerarse acto técnico o "norma encubierta" sujeta a publicidad normativa y control jurisdiccional pleno.</dd>

        <dt>Auditoría algorítmica</dt>
        <dd>Procedimiento técnico-jurídico para evaluar el funcionamiento, sesgos, riesgos y cumplimiento de un sistema de IA. Exigida para sistemas de alto riesgo por el AI Act. En España, vinculada al buen gobierno, control interno y responsabilidad patrimonial.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, arts. 19 y ss.</a></dd>

        <dt>Automatización de decisiones</dt>
        <dd>Proceso donde una decisión se adopta sin intervención humana significativa. Vinculada al art. 22 del RGPD y a los principios de supervisión humana del AI Act.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32016R0679">RGPD, art. 22</a>; <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, art. 14</a>.</dd>

        <dt>AESIA (Agencia Española de Supervisión de la Inteligencia Artificial)</dt>
        <dd>Organismo público español encargado de supervisar el desarrollo y uso de la IA, garantizar su aplicación ética, segura e inclusiva, y coordinar la aplicación del AI Act en España. Sede en A Coruña; funciones incluyen supervisión normativa, atención a la sociedad, formación y colaboración institucional.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://aesia.digital.gob.es/es">Sitio oficial AESIA</a>; <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, art. 70</a>.<br>
        <strong>Notas 2026:</strong> Plena operativa con guías publicadas y liderazgo en entornos sandbox.</dd>
    </dl>
</section>

<section id="B">
    <h2>B</h2>
    <dl>
        <dt>Bias (sesgo algorítmico)</dt>
        <dd>Desviación sistemática en los resultados de la IA que genera efectos discriminatorios. Se conecta directamente con el derecho antidiscriminatorio, la igualdad ante la ley (art. 14 CE) y la protección de derechos fundamentales (art. 21 Carta de Derechos Fundamentales UE).<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, art. 10</a>; <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32016R0679">RGPD, art. 5</a>.</dd>

        <dt>Buen gobierno algorítmico</dt>
        <dd>Aplicación de los principios de legalidad, eficacia, transparencia y objetividad al uso de IA por poderes públicos. Tiene su base constitucional en el art. 103 CE (objetividad y sometimiento pleno a la ley).</dd>
    </dl>
</section>

<section id="C">
    <h2>C</h2>
    <dl>
        <dt>Carta de Derechos Digitales (España)</dt>
        <dd>Documento de 2021 que reconoce derechos en el entorno digital. Aunque no tiene rango de ley, posee valor interpretativo e influye en políticas públicas. Incluye el derecho a la transparencia algorítmica y a no ser objeto de decisiones exclusivamente automatizadas sin garantías.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://www.lamoncloa.gob.es/presidente/actividades/Documents/2021/140721-Carta_Derechos_Digitales_RedEs.pdf">Texto oficial</a>.</dd>

        <dt>Carga de la prueba algorítmica</dt>
        <dd>Problema procesal sobre quién debe probar el funcionamiento del sistema o la ausencia de sesgos, fundamental en litigios por discriminación.</dd>

        <dt>Constitucionalismo digital</dt>
        <dd>Corriente doctrinal que defiende la reinterpretación de principios clásicos (dignidad, separación de poderes) para gobernar tecnologías como la IA. Considera la IA como un problema constitucional y no meramente regulatorio.</dd>

        <dt>Control jurisdiccional de la IA</dt>
        <dd>Capacidad de los tribunales (civil, social o contencioso-administrativo) para revisar decisiones basadas en IA. Se enfrenta a la dificultad de la opacidad técnica y la asimetría informativa.</dd>

        <dt>Cumplimiento normativo (compliance algorítmico)</dt>
        <dd>Mecanismos técnicos y organizativos para asegurar que la IA cumple con el AI Act, RGPD y la Carta de Derechos Fundamentales de la UE. Incluye auditorías y evaluación de riesgos.</dd>
    </dl>
</section>

<section id="D">
    <h2>D</h2>
    <dl>
        <dt>Datos de entrenamiento</dt>
        <dd>Conjunto de datos usados para entrenar modelos de IA. Son críticos jurídicamente por la protección de datos personales, la propiedad intelectual y los sesgos estructurales que puedan contener.</dd>

        <dt>Debido proceso algorítmico</dt>
        <dd>Extensión de las garantías procedimentales a las decisiones automatizadas, incluyendo el derecho a la explicación, impugnación y garantías frente a sistemas opacos.</dd>

        <dt>Derecho a la explicación</dt>
        <dd>Derecho del afectado a recibir información significativa sobre la lógica, significado y consecuencias de un sistema automatizado.<br>
        <span class="ref">Fuentes:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32016R0679">RGPD, art. 22</a>. En España vinculado a los arts. 24 CE (tutela efectiva) y 105 CE (acceso a archivos).</dd>

        <dt>Discrecionalidad técnica algorítmica</dt>
        <dd>Uso de IA para apoyar decisiones discrecionales administrativas, planteando si reduce la motivación exigible o sustituye la voluntad humana.</dd>
    </dl>
</section>

<section id="E">
    <h2>E</h2>
    <dl>
        <dt>Evaluación de impacto algorítmico / en derechos fundamentales (FRIA)</dt>
        <dd>Análisis previo para identificar efectos sobre derechos fundamentales en sistemas de alto riesgo. Tipos: DPIA (datos personales) y FRIA (derechos fundamentales). En España asociada al principio de precaución.<br>
        <span class="ref">Referencia:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, Capítulo III (sistemas de alto riesgo)</a>.</dd>

        <dt>Explicabilidad / Explainability</dt>
        <dd>Capacidad de un sistema para ofrecer una explicación comprensible, relacionada con la transparencia y legitimidad.</dd>

        <dt>Explicabilidad jurídica</dt>
        <dd>Capacidad de ofrecer una explicación que permita impugnar, defender y controlar judicialmente una decisión, más allá de la explicación técnica del ingeniero.</dd>
    </dl>
</section>

<section id="F">
    <h2>F</h2>
    <dl>
        <dt>Fairness (equidad algorítmica)</dt>
        <dd>Principio que exige que la IA trate a las personas de forma justa y no discriminatoria. Se basa en la Carta de Derechos Fundamentales de la UE (art. 21) y el art. 14 CE.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:12012P/TXT">Carta de Derechos Fundamentales de la UE, art. 21</a>; <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, art. 10</a>.</dd>

        <dt>Fallo algorítmico</dt>
        <dd>Resultado ilícito o incorrecto de la IA que genera responsabilidad civil objetiva o responsabilidad patrimonial de la Administración (art. 106.2 CE).</dd>
    </dl>
</section>

<section id="G">
    <h2>G</h2>
    <dl>
        <dt>Gobernanza de la IA</dt>
        <dd>Normas, instituciones (autoridades de supervisión) y soft law que regulan el ciclo de vida de la IA.</dd>

        <dt>Gobernanza multinivel de la IA</dt>
        <dd>Coordinación entre la UE, Estados miembros y autoridades locales para aplicar regulaciones como el AI Act. Crea la AI Office para este fin.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, art. 64 (AI Office)</a>.</dd>
    </dl>
</section>

<section id="H">
    <h2>H</h2>
    <dl>
        <dt>High-Risk AI System (Sistema de IA de alto riesgo)</dt>
        <dd>Sistemas que afectan ámbitos como justicia, empleo, educación o seguridad y están sujetos a obligaciones reforzadas (evaluación de conformidad, registro, etc.).<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, Anexo III</a>.</dd>

        <dt>Human-in-the-loop / Supervisión humana</dt>
        <dd>Capacidad real de control y corrección de un sistema por parte de un humano, no una intervención meramente simbólica.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, art. 14</a>.</dd>
    </dl>
</section>

<section id="I">
    <h2>I</h2>
    <dl>
        <dt>IA confiable (Trustworthy AI)</dt>
        <dd>Concepto de la UE que exige una IA legal, ética y robusta. Se fundamenta en el respeto a los derechos fundamentales y la seguridad.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, art. 1</a>.</dd>

        <dt>IA generativa</dt>
        <dd>Sistemas que producen contenidos (texto, imágenes, etc.), planteando retos en propiedad intelectual, responsabilidad y desinformación.</dd>

        <dt>IA en la Administración Pública</dt>
        <dd>Uso de IA para gestión, ayudas e inspección; el límite es que no puede sustituir la responsabilidad del órgano administrativo.</dd>

        <dt>Impacto en derechos fundamentales</dt>
        <dd>Afectación de la IA a la igualdad, intimidad, protección de datos y tutela judicial efectiva.</dd>
    </dl>
</section>

<section id="L">
    <h2>L</h2>
    <dl>
        <dt>Legal Tech</dt>
        <dd>Aplicación de tecnología al Derecho (análisis de jurisprudencia, redacción automática, etc.). No toda Legal Tech es IA de alto riesgo.</dd>

        <dt>Legalidad algorítmica</dt>
        <dd>Principio de que ninguna decisión automatizada puede carecer de base legal y garantías procesales.<br>
        <span class="ref">Referencia:</span> Constitución Española, arts. 9.3, 14 y 24.</dd>

        <dt>Legitimidad democrática de la IA</dt>
        <dd>Necesidad de control parlamentario y transparencia en el uso público de la IA.</dd>
    </dl>
</section>

<section id="M">
    <h2>M</h2>
    <dl>
        <dt>Modelo de lenguaje (LLM)</dt>
        <dd>Sistemas entrenados con grandes volúmenes de datos que presentan riesgos de "alucinaciones" y falta de trazabilidad.</dd>

        <dt>Motivación del acto algorítmico</dt>
        <dd>Obligación de justificar decisiones apoyadas en IA según la Ley 39/2015.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://www.boe.es/buscar/act.php?id=BOE-A-2015-10565">Ley 39/2015</a>.</dd>
    </dl>
</section>

<section id="O">
    <h2>O</h2>
    <dl>
        <dt>Opacidad algorítmica / estructural</dt>
        <dd>Dificultad para comprender cómo la IA llega a un resultado ("caja negra"), lo que afecta la transparencia, la prueba y el control judicial.</dd>
    </dl>
</section>

<section id="P">
    <h2>P</h2>
    <dl>
        <dt>Prácticas prohibidas</dt>
        <dd>Prácticas de IA de riesgo inaceptable vedadas por incompatibles con valores UE (ej. social scoring, manipulación subliminal). Vigentes desde 02.02.2025.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, art. 5</a>.</dd>

        <dt>Principio de proporcionalidad</dt>
        <dd>Exige que el uso de IA sea necesario y equilibrado respecto a los fines perseguidos.</dd>

        <dt>Principio de precaución tecnológica</dt>
        <dd>Aplicación de la precaución ante riesgos inciertos en IA predictiva o policial.</dd>

        <dt>Proveedor vs Deployer / Usuario</dt>
        <dd>Proveedor: desarrolla y pone en mercado el sistema. Deployer/Usuario: lo utiliza bajo su responsabilidad. Obligaciones diferenciadas.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, art. 3</a>.</dd>

        <dt>Sandbox regulatorio</dt>
        <dd>Entorno controlado para probar IA innovadora con exenciones temporales y supervisión. En España impulsado por AESIA.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, art. 57</a>.</dd>

        <dt>Sistema de IA</dt>
        <dd>Sistema basado en máquina que opera con autonomía, genera outputs (predicciones, recomendaciones, decisiones) y puede adaptarse tras despliegue.<br>
        <span class="ref">Referencia oficial:</span> <a href="https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689">Reglamento (UE) 2024/1689, art. 3(1)</a>.</dd>
    </dl>
</section>

<section id="R">
    <h2>R</h2>
    <dl>
        <dt>Reserva de humanidad</dt>
        <dd>Doctrina que establece que decisiones sensibles (sanciones, justicia penal) deben ser tomadas exclusivamente por personas. Vinculada al RGPD y al AI Act.</dd>

        <dt>Responsabilidad algorítmica</dt>
        <dd>Debate sobre quién responde por los daños causados por la IA (modelos objetivos o compartidos).</dd>

        <dt>Responsabilidad patrimonial por IA</dt>
        <dd>Régimen de responsabilidad objetiva de la Administración por daños causados por sistemas automatizados (art. 106.2 CE).</dd>
    </dl>
</section>

<section id="S">
    <h2>S</h2>
    <dl>
        <dt>Soberanía digital</dt>
        <dd>Capacidad del Estado y la UE para controlar tecnologías críticas y desarrollar IA pública internamente.</dd>

        <dt>Soft law</dt>
        <dd>Instrumentos no vinculantes (códigos de conducta, guías) esenciales en la gobernanza de la IA.</dd>
    </dl>
</section>

<section id="T">
    <h2>T</h2>
    <dl>
        <dt>Transparencia algorítmica</dt>
        <dd>Obligación de informar sobre el uso y finalidad de la IA para permitir el control y la rendición de cuentas.</dd>

        <dt>Tutela judicial efectiva frente a la IA</dt>
        <dd>Derecho (art. 24 CE) a que un juez pueda revisar y anular una decisión automatizada ilegal con las mismas garantías que una decisión humana.</dd>
    </dl>
</section>

<section id="V">
    <h2>V</h2>
    <dl>
        <dt>Valores de la Unión Europea</dt>
        <dd>Marco axiológico (art. 2 TUE) que guía la regulación: dignidad, libertad, democracia e igualdad.</dd>

        <dt>Vigilancia algorítmica</dt>
        <dd>Uso de IA para monitorizar o predecir comportamientos, siendo un ámbito de alto riesgo para los derechos fundamentales.</dd>
    </dl>
</section>

<section id="X">
    <h2>X</h2>
    <dl>
        <dt>XAI (Explainable Artificial Intelligence)</dt>
        <dd>Campo técnico orientado a desarrollar sistemas explicables para cumplir con obligaciones legales de transparencia y control.</dd>
    </dl>
</section>

<footer>
    <p>Fuentes principales: EUR-Lex, BOE, AESIA[](https://aesia.digital.gob.es/es). Última actualización: febrero 2026.<br>
    Este glosario es informativo y no sustituye asesoramiento legal específico. © Derecho Artificial.</p>
</footer>

</body>
</html>